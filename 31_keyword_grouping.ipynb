{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import asyncio\n",
    "import copy\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from functions import load_json\n",
    "from prompts import group_kw\n",
    "from keys import openai_key\n",
    "\n",
    "client = AsyncOpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json file from data folder\n",
    "transcripts_4o = load_json('/workspaces/youtube-ad-detection/data/kw_transcripts_gpt4o.json')\n",
    "transcripts_4 = load_json('/workspaces/youtube-ad-detection/data/kw_transcripts_gpt4.json')\n",
    "\n",
    "ads_4o = load_json('/workspaces/youtube-ad-detection/data/kw_ads_gpt4o.json')\n",
    "ads_4 = load_json('/workspaces/youtube-ad-detection/data/kw_ads_gpt4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def group_keywords(model, system_prompt, content, type):\n",
    "    system_msg = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    user_assistant_msgs = [\n",
    "        {   \"role\": \"user\",\n",
    "            \"content\": repr(content['metadata']['kw'][type])\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    msgs = system_msg + user_assistant_msgs\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=msgs,\n",
    "                temperature=0\n",
    "                )\n",
    "    \n",
    "    return {\n",
    "        'videoId': content['metadata']['videoId'],\n",
    "        'kw' : str(response.choices[0].message.content)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_contents(model, system_prompt, all_content, type, max_concurrent_tasks=10):\n",
    "    # Create a semaphore to control concurrency\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_tasks)\n",
    "    \n",
    "    # Define an async function to wrap find_ads and include semaphore control\n",
    "    async def process_single_content(content):\n",
    "        async with semaphore:\n",
    "            return await group_keywords(model, system_prompt, content, type)\n",
    "    \n",
    "    # List to hold the tasks\n",
    "    tasks = []\n",
    "    results = []\n",
    "\n",
    "    for index, value in enumerate(all_content):\n",
    "        tasks.append(asyncio.create_task(process_single_content(value)))\n",
    "        \n",
    "        # If we've reached the batch size or the end of the list, wait for the current batch to finish\n",
    "        if (index + 1) % max_concurrent_tasks == 0 or (index + 1) == len(all_content):\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "            results.extend(batch_results)\n",
    "            tasks.clear()  # Clear the task list for the next batch\n",
    "\n",
    "            # Introduce delay after processing each batch, except after the final batch\n",
    "            if (index + 1) < len(all_content):\n",
    "                await asyncio.sleep(5)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_contents_loop(data, model, system_prompt):\n",
    "    output = copy.deepcopy(data)\n",
    "    for channel, videos in output.items():\n",
    "        print(f\"Processing {channel}\")\n",
    "        for transcript_type in ['manual', 'generated']:\n",
    "            results = await process_all_contents(model, system_prompt, videos, transcript_type)\n",
    "            for i in results:\n",
    "                # find the matching dictionary in output\n",
    "                for video in output[channel]:\n",
    "                    if video['metadata']['videoId'] == i['videoId']:\n",
    "                        video['metadata']['gpt'] = {\n",
    "                            'generated': '',\n",
    "                            'manual': ''\n",
    "                        }\n",
    "                        video['metadata']['gpt'][transcript_type] = i['kw']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-4o-2024-08-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SciShow\n",
      "Processing Johnny Harris\n",
      "Processing PBS Space Time\n",
      "Processing 3Blue1Brown\n",
      "Processing DamiLee\n",
      "Processing Fireship\n",
      "Processing SciShow\n",
      "Processing Johnny Harris\n",
      "Processing PBS Space Time\n",
      "Processing 3Blue1Brown\n",
      "Processing DamiLee\n",
      "Processing Fireship\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82846/832052506.py:3: RuntimeWarning: coroutine 'process_all_contents_loop' was never awaited\n",
      "  kw_gpt_transcripts_gpt4 = await process_all_contents_loop(transcripts_4, model, group_kw)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "model = 'gpt-4o-2024-08-06'\n",
    "kw_gpt_transcripts_gpt4o = await process_all_contents_loop(transcripts_4o, model, group_kw)\n",
    "kw_gpt_ads_gpt4o = await process_all_contents_loop(ads_4o, model, group_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_transcripts_4o = copy.deepcopy(kw_gpt_transcripts_gpt4o)\n",
    "ori_ads_4o = copy.deepcopy(kw_gpt_ads_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_gpt_transcripts_gpt4o = copy.deepcopy(ori_transcripts_4o)\n",
    "kw_gpt_ads_gpt4o = copy.deepcopy(ori_ads_4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list representation\n",
    "for data in [kw_gpt_transcripts_gpt4o, kw_gpt_ads_gpt4o]:\n",
    "    for channel, videos in data.items():\n",
    "        for video in videos:\n",
    "            for transcript_type in ['manual', 'generated']:\n",
    "                if video['metadata']['gpt'][transcript_type] and type(video['metadata']['gpt'][transcript_type])==str:\n",
    "                    try:\n",
    "                        reformat_list = video['metadata']['gpt'][transcript_type].replace('_', ' ')\n",
    "                        video['metadata']['gpt'][transcript_type] = list(set(ast.literal_eval(reformat_list)))\n",
    "                    except:\n",
    "                        elements = video['metadata']['gpt'][transcript_type]\\\n",
    "                                    .replace('_', ' ')\\\n",
    "                                    .replace(\"'\", '')\\\n",
    "                                    .strip(\"[]\").split(\", \")\n",
    "                        quoted_elements = [f'\"{element}\"' for element in elements]\n",
    "                        reformat_list = \"[\" + \", \".join(quoted_elements) + \"]\"\n",
    "                        video['metadata']['gpt'][transcript_type] = list(set(ast.literal_eval(reformat_list)))\n",
    "                else:\n",
    "                    video['metadata']['gpt'][transcript_type] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed to json file\n",
    "with open('/workspaces/youtube-ad-detection/data/kw_gpt_transcripts_gpt4o.json', 'w') as f:\n",
    "    json.dump(kw_gpt_transcripts_gpt4o, f, indent=4)\n",
    "\n",
    "with open('/workspaces/youtube-ad-detection/data/kw_gpt_ads_gpt4o.json', 'w') as f:\n",
    "    json.dump(kw_gpt_ads_gpt4o, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-4-turbo-2024-04-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SciShow\n",
      "Processing Johnny Harris\n",
      "Processing PBS Space Time\n",
      "Processing 3Blue1Brown\n",
      "Processing DamiLee\n",
      "Processing Fireship\n",
      "Processing SciShow\n",
      "Processing Johnny Harris\n",
      "Processing PBS Space Time\n",
      "Processing 3Blue1Brown\n",
      "Processing DamiLee\n",
      "Processing Fireship\n"
     ]
    }
   ],
   "source": [
    "model = 'gpt-4o-2024-08-06'\n",
    "kw_gpt_transcripts_gpt4 = await process_all_contents_loop(transcripts_4, model, group_kw)\n",
    "kw_gpt_ads_gpt4 = await process_all_contents_loop(ads_4, model, group_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list representation\n",
    "for data in [kw_gpt_transcripts_gpt4, kw_gpt_ads_gpt4]:\n",
    "    for channel, videos in data.items():\n",
    "        for video in videos:\n",
    "            for transcript_type in ['manual', 'generated']:\n",
    "                if video['metadata']['gpt'][transcript_type] and type(video['metadata']['gpt'][transcript_type])==str:\n",
    "                    try:\n",
    "                        reformat_list = video['metadata']['gpt'][transcript_type].replace('_', ' ')\n",
    "                        video['metadata']['gpt'][transcript_type] = ast.literal_eval(reformat_list)\n",
    "                    except:\n",
    "                        elements = video['metadata']['gpt'][transcript_type]\\\n",
    "                                    .replace('_', ' ')\\\n",
    "                                    .replace(\"'\", '')\\\n",
    "                                    .strip(\"[]\").split(\", \")\n",
    "                        quoted_elements = [f'\"{element}\"' for element in elements]\n",
    "                        reformat_list = \"[\" + \", \".join(quoted_elements) + \"]\"\n",
    "                        video['metadata']['gpt'][transcript_type] = ast.literal_eval(reformat_list)\n",
    "                else:\n",
    "                    video['metadata']['gpt'][transcript_type] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed to json file\n",
    "with open('/workspaces/youtube-ad-detection/data/kw_gpt_transcripts_gpt4.json', 'w') as f:\n",
    "    json.dump(kw_gpt_transcripts_gpt4, f, indent=4)\n",
    "\n",
    "with open('/workspaces/youtube-ad-detection/data/kw_gpt_ads_gpt4.json', 'w') as f:\n",
    "    json.dump(kw_gpt_ads_gpt4, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
